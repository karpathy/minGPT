{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a character-level GPT on some text data\n",
    "\n",
    "The inputs here are simple text files, which we chop up to individual characters and then train GPT on. So you could say this is a char-transformer instead of a char-rnn. Doesn't quite roll off the tongue as well. In this example we will feed it some shakespear, which we'll get it to predict character-level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make deterministic\n",
    "from pytorch_lightning import seed_everything\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CharDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data, block_size):\n",
    "        chars = list(set(data))\n",
    "        data_size, vocab_size = len(data), len(chars)\n",
    "        print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "\n",
    "        self.stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "        self.itos = { i:ch for i,ch in enumerate(chars) }\n",
    "        self.block_size = block_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return math.ceil(len(self.data) / (self.block_size + 1))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # we're actually going to \"cheat\" and pick a spot in the dataset at random\n",
    "        i = np.random.randint(0, len(self.data) - (self.block_size + 1))\n",
    "        chunk = self.data[i:i+self.block_size+1]\n",
    "        dix = [self.stoi[s] for s in chunk]\n",
    "        x = torch.tensor(dix[:-1], dtype=torch.long)\n",
    "        y = torch.tensor(dix[1:], dtype=torch.long)\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 128 # spatial extent of the model for its context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-08-19 16:03:55--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 199.232.64.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|199.232.64.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: ‘input.txt’\n",
      "\n",
      "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.03s   \n",
      "\n",
      "2020-08-19 16:03:55 (42.3 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# download text from \n",
    "! wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 1115394 characters, 65 unique.\n"
     ]
    }
   ],
   "source": [
    "# you can download this file at https://github.com/karpathy/char-rnn/blob/master/data/tinyshakespeare/input.txt\n",
    "text = open('input.txt', 'r').read() # don't worry we won't run out of file handles\n",
    "train_dataset = CharDataset(text, block_size) # one line of poem is roughly 50 characters\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mingpt.model import GPT\n",
    "model = GPT(vocab_size=train_dataset.vocab_size, \n",
    "            block_size=train_dataset.block_size,\n",
    "            n_layer=8, \n",
    "            n_head=8, \n",
    "            n_embd=512, \n",
    "            learning_rate=6e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "CUDA_VISIBLE_DEVICES: [0]\n",
      "Using native 16bit precision.\n",
      "\n",
      "  | Name    | Type       | Params\n",
      "---------------------------------------\n",
      "0 | tok_emb | Embedding  | 33 K  \n",
      "1 | drop    | Dropout    | 0     \n",
      "2 | blocks  | Sequential | 25 M  \n",
      "3 | ln_f    | LayerNorm  | 1 K   \n",
      "4 | head    | Linear     | 33 K  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab86a3945eb54225b221c2a07f00509f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:23: UserWarning: \n",
      "                    When using EvalResult(early_stop_on=X) or TrainResult(early_stop_on=X) the\n",
      "                    'monitor' key of ModelCheckpoint has no effect.\n",
      "                    Remove ModelCheckpoint(monitor='loss) to fix')\n",
      "                \n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:23: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "from mingpt.lr_decay import LearningRateDecayCallback\n",
    "\n",
    "# scheduler\n",
    "lr_decay = LearningRateDecayCallback(learning_rate=6e-4, warmup_tokens=512*20,\n",
    "                                    final_tokens=00*len(train_dataset)*block_size)\n",
    "\n",
    "trainer = Trainer(gpus=1, precision=16, max_epochs=500,\n",
    "                  gradient_clip_val=1.0, \n",
    "                  callbacks=[lr_decay], \n",
    "                  progress_bar_refresh_rate=1, \n",
    "                  row_log_interval=1)\n",
    "trainer.fit(model, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O God, I code but their friends.\n",
      "\n",
      "KING EDWARD IV:\n",
      "Thou hast thronge indable thy father friar,\n",
      "Stand up and desperite, should virtuous advit.\n",
      "\n",
      "SICINIUS:\n",
      "Sir, the king of words this land him.\n",
      "\n",
      "BIONDA:\n",
      "You marry; my lord.\n",
      "\n",
      "SICINIUS:\n",
      " faith, know, you say, My company.\n",
      "\n",
      "MENENIUS:\n",
      "You passion, this name:\n",
      "If she do seat your sight, and no more,\n",
      "So save man than still, what says 'tis more commongt\n",
      "To sling hell bit will be bastanded of your deliver,\n",
      "Remither than shall still, his land hand;\n",
      "More im thou not, and the subject more,\n",
      "Stime at eample, and saffe his corder--feath, this\n",
      "manify stiff his life, and what may live, and\n",
      "Nor what shorn compassion to my sover; but I do,\n",
      "I'll commplainly to still, be born him: I am thought\n",
      "In shhe yould still, and say 'anoth;\n",
      "For though here do selfs and consul,\n",
      "With leave more brings and ours, catisfied,\n",
      "Shaill and yourself to your most to think,\n",
      "Where believes their dince and thou ne'er to kithfull;\n",
      "With hom they have do your high earth to thing,\n",
      "Which shall hm lear him ca\n"
     ]
    }
   ],
   "source": [
    "# alright, let's sample some character-level shakespear\n",
    "from mingpt.utils import sample\n",
    "\n",
    "context = \"O God, I code but\"\n",
    "x = torch.tensor([train_dataset.stoi[s] for s in context], dtype=torch.long)[None,...].to(model.device)\n",
    "y = sample(model, x, 1000, temperature=0.9, sample=True, top_k=5)[0]\n",
    "completion = ''.join([train_dataset.itos[int(i)] for i in y])\n",
    "print(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# well that was fun..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
